{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "EVZbTfBslbWa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"GPU is available with {num_gpus} device(s).\")\n",
    "    \n",
    "    # Get the name of the current GPU\n",
    "    current_gpu = torch.cuda.get_device_name(0)  # Assuming the first GPU is used\n",
    "    print(f\"Current GPU: {current_gpu}\")\n",
    "else:\n",
    "    print(\"GPU is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use the first available GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # If no GPU is available, use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "models_dir = current_directory + '/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarPanelDataset(data.Dataset):\n",
    "\n",
    "    #data_dir: The directory containing the image files\n",
    "    #image_shape: The shape of the images (height, width, channels)\n",
    "    #augmentation_copies: The number of copies of each image to create for data augmentation\n",
    "\n",
    "    def __init__(self, df, data_dir, image_shape=(3, 256, 256), augmentation_copies=3):\n",
    "\n",
    "        self.copies = 1 + augmentation_copies\n",
    "        self.image_shape = image_shape\n",
    "        self.df = df #df: A pandas DataFrame containing the image filenames and labels\n",
    "        self.labels = []\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.augmentation_transform = self.make_augmentation_transform()\n",
    "        self.dataset_size = len(df)\n",
    "        self.images = torch.zeros((self.dataset_size, *self.image_shape), dtype=torch.float32)\n",
    "\n",
    "        for i, img_name in enumerate(self.df['original_title']):\n",
    "            img_path = os.path.join(data_dir, img_name)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            self.images[i] = image\n",
    "            label = df.iloc[i]['loss_percentage']\n",
    "            self.labels.append(label)  # Add corresponding label here\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)*self.copies\n",
    "\n",
    "    def make_augmentation_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop((self.image_shape[1], self.image_shape[2])),\n",
    "            transforms.RandomHorizontalFlip(p=0.2),\n",
    "            transforms.RandomVerticalFlip(p=0.2),\n",
    "            transforms.RandomRotation(45),\n",
    "            transforms.ColorJitter(brightness=(0.3, 2), contrast=0, saturation=0, hue=0),\n",
    "            #transforms.RandomGrayscale(p=0.1)\n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if (idx >= len(self.images) ):\n",
    "            image = self.images[idx % len(self.images)]\n",
    "            label = self.labels[idx % len(self.labels)]\n",
    "            return self.augmentation_transform(image), torch.tensor(label, dtype=torch.float32)\n",
    "            \n",
    "            \n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfessorsCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(59536, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to save the model\n",
    "def saveModel(model:nn.Module, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Training function. We simply have to loop over our data iterator and feed the inputs to the network and optimize.\n",
    "def train(model: torch.nn.Module,\n",
    "          dataloader: torch.utils.data.DataLoader,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          metric: torchmetrics.Metric,\n",
    "          device: torch.device,\n",
    "          num_epochs,\n",
    "          path_model,\n",
    "          report: pd.DataFrame,\n",
    "          verbatim):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    metric.to(device)\n",
    "    best_accuracy = 0.0\n",
    "    best_loss = 0.0\n",
    "    best_epoch = 0\n",
    "    metric_algorithm = metric.__class__.__name__\n",
    "    loss_algorithm = loss_fn.__class__.__name__\n",
    "    best_metric = 100\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        for ibatch, (images, labels) in enumerate(dataloader, 0):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(images)\n",
    "            y_pred = y_pred.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            metric(y_pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(loss)\n",
    "            # Calculate and accumulate accuracy metric across all batches\n",
    "            #y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            #train_acc += (y_pred_class == labels).sum().item()/len(y_pred)\n",
    "\n",
    "        # Adjust metrics to get average loss and accuracy per batch\n",
    "        train_loss = train_loss / len(dataloader)\n",
    "\n",
    "        # we want to save the model if the accuracy is the best\n",
    "\n",
    "        path = \"./myModel_\" +str(epoch)+ \".pth\"\n",
    "        #saveModel(model, path = path)\n",
    "\n",
    "        batch_metric = metric.compute()\n",
    "        report_line = {'epoch': epoch,\n",
    "                       'batch_loss': train_loss,\n",
    "                       'metric_algorithm': metric_algorithm,\n",
    "                       'batch_metric': batch_metric.item(),\n",
    "                       'loss_algorithm': loss_algorithm\n",
    "                      }\n",
    "        report = pd.concat([report, pd.DataFrame([report_line])], ignore_index=True)\n",
    "\n",
    "        if batch_metric < best_metric:\n",
    "            path = str(path_model + '/bestModel.pth')\n",
    "            saveModel(model, path = path)\n",
    "            best_loss = train_loss\n",
    "            best_metric = batch_metric\n",
    "            best_epoch = epoch\n",
    "            if verbatim:\n",
    "              print('Best Epoch #', epoch,' Loss=', best_loss, \" Accu=\", best_accuracy )\n",
    "\n",
    "    return best_loss, best_accuracy, best_epoch, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/mestrado/cv/virtualenv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2195, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2211, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1480, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1420, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1559, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1120, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0735, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0942, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0809, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0952, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0922, grad_fn=<MseLossBackward0>)\n",
      "Best Epoch # 0  Loss= 0.12136906385421753  Accu= 0.0\n",
      "tensor(0.1021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1050, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0861, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0743, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1127, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1026, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1174, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1349, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = ProfessorsCNN()\n",
    "metric = torchmetrics.MeanAbsolutePercentageError()\n",
    "dataset = SolarPanelDataset(train_df.sample(200), '/home/victor/mestrado/cv/project/CV-Project/SolarPanelSoilingImageDataset/Solar_Panel_Soiling_Image_dataset/PanelImages/')\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "loss_fn = torch.nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "num_epochs = 2\n",
    "path_model = models_dir + f'/{model.__class__.__name__}_{loss_fn.__class__.__name__}'\n",
    "os.makedirs(path_model, exist_ok=True)\n",
    "report = pd.DataFrame()\n",
    "_, _, _, report = train(model, train_dataloader, loss_fn, optimizer, metric, device, num_epochs, path_model, report, verbatim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model with the test dataset and print the accuracy for the test images\n",
    "def test(model: torch.nn.Module,\n",
    "         dataloader: torch.utils.data.DataLoader,\n",
    "         loss_fn: torch.nn.Module,\n",
    "         metric: torchmetrics.Metric,\n",
    "         device: torch.device,\n",
    "         verbatim = True):\n",
    "\n",
    "    model.eval()\n",
    "    metric = metric.to(device)\n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss = 0\n",
    "    test_metric = 100\n",
    "    pred_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            y_pred = model(images)\n",
    "            y_pred = y_pred.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            test_loss += loss.item()\n",
    "            metric.update(y_pred, labels)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_metric = metric.compute()\n",
    "\n",
    "    if verbatim:\n",
    "      print(\"Loss =\", test_loss, f'  Metric ({metric.__class__.__name__})=', test_metric.item())\n",
    "    return pred_labels, test_loss, test_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.08562727957963943   Metric (MeanAbsolutePercentageError)= 18.312280654907227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], 0.08562727957963943, tensor(18.3123))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = torchmetrics.MeanAbsolutePercentageError()\n",
    "test_dataset = SolarPanelDataset(test_df.sample(200), '/home/victor/mestrado/cv/project/CV-Project/SolarPanelSoilingImageDataset/Solar_Panel_Soiling_Image_dataset/PanelImages/')\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "test(model, test_dataloader, loss_fn, metric, device, verbatim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "8rpJuShwm6wk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "iIKHssGLogv5"
   },
   "outputs": [],
   "source": [
    "\n",
    "def tensor_to_image(tensor):\n",
    "\n",
    "    \"\"\"\n",
    "    Converts a tensor to an image.\n",
    "\n",
    "    Args:\n",
    "        tensor: A tensor of shape (C, H, W).\n",
    "\n",
    "    Returns:\n",
    "        An image.\n",
    "    \"\"\"\n",
    "\n",
    "    image = tensor.cpu().detach().numpy()\n",
    "    image = image.transpose(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    image = (image * 255).astype('uint8')  # Convert pixel values from [0, 1] to [0, 255]\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vYUgNJypt8t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
