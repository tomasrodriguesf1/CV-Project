{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "EVZbTfBslbWa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"GPU is available with {num_gpus} device(s).\")\n",
    "    \n",
    "    # Get the name of the current GPU\n",
    "    current_gpu = torch.cuda.get_device_name(0)  # Assuming the first GPU is used\n",
    "    print(f\"Current GPU: {current_gpu}\")\n",
    "else:\n",
    "    print(\"GPU is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use the first available GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # If no GPU is available, use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "models_dir = current_directory + '/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarPanelDataset(data.Dataset):\n",
    "    def __init__(self, df, data_dir, image_shape=(3, 256, 256), augmentation_copies=3):\n",
    "        self.copies = 1+augmentation_copies\n",
    "        self.image_shape = image_shape\n",
    "        self.df = df\n",
    "        self.labels = []\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.augmentation_transform = self.make_augmentation_transform()\n",
    "        self.dataset_size = len(df)\n",
    "        self.images = torch.zeros((self.dataset_size, *self.image_shape), dtype=torch.float32)\n",
    "        for i, img_name in enumerate(self.df['original_title']):\n",
    "            img_path = os.path.join(data_dir, img_name)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            self.images[i] = image\n",
    "            label = df.iloc[i]['loss_percentage']\n",
    "            self.labels.append(label)  # Add corresponding label here\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)*self.copies\n",
    "\n",
    "    def make_augmentation_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop((self.image_shape[1], self.image_shape[2])),\n",
    "            transforms.RandomHorizontalFlip(p=0.2),\n",
    "            transforms.RandomVerticalFlip(p=0.2),\n",
    "            transforms.RandomRotation(45),\n",
    "            transforms.ColorJitter(brightness=(0.3, 2), contrast=0, saturation=0, hue=0),\n",
    "            #transforms.RandomGrayscale(p=0.1)\n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if (idx >= len(self.images) ):\n",
    "            image = self.images[idx % len(self.images)]\n",
    "            label = self.labels[idx % len(self.labels)]\n",
    "            return self.augmentation_transform(image), torch.tensor(label, dtype=torch.float32)\n",
    "            \n",
    "            \n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfessorsCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(59536, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to save the model\n",
    "def saveModel(model:nn.Module, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Training function. We simply have to loop over our data iterator and feed the inputs to the network and optimize.\n",
    "def train(model: torch.nn.Module,\n",
    "          dataloader: torch.utils.data.DataLoader,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          device: torch.device,\n",
    "          num_epochs,\n",
    "          path_model,\n",
    "          verbatim):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    best_accuracy = 0.0\n",
    "    best_loss = 0.0\n",
    "    best_epoch = 0\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        for ibatch, (images, labels) in enumerate(dataloader, 0):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(images)\n",
    "            y_pred = y_pred.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 3. Optimizer zero grad\n",
    "            # ...\n",
    "\n",
    "            # 4. Loss backward\n",
    "            # ...\n",
    "\n",
    "            # 5. Optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            print(loss)\n",
    "            # Calculate and accumulate accuracy metric across all batches\n",
    "            #y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            #train_acc += (y_pred_class == labels).sum().item()/len(y_pred)\n",
    "\n",
    "        print(train_loss)\n",
    "        # Adjust metrics to get average loss and accuracy per batch\n",
    "        train_loss = train_loss / len(dataloader)\n",
    "        #train_acc = train_acc / len(dataloader)\n",
    "\n",
    "        # we want to save the model if the accuracy is the best\n",
    "\n",
    "        path = \"./myModel_\" +str(epoch)+ \".pth\"\n",
    "        saveModel(model, path = path)\n",
    "\n",
    "\n",
    "        #if train_acc > best_accuracy:\n",
    "        #    path = str(path_model)\n",
    "        #    saveModel(model, path = path)\n",
    "        #    best_loss = train_loss\n",
    "        #    best_accuracy = train_acc\n",
    "        #    best_epoch = epoch\n",
    "        #    if verbatim:\n",
    "        #      print('Best Epoch #', epoch,' Loss=', best_loss, \" Accu=\", best_accuracy )\n",
    "\n",
    "    return best_loss, best_accuracy, best_epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1181, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1052, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0865, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0499, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0942, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0793, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0983, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0929, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1033, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0942, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0722, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0722, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0649, grad_fn=<MseLossBackward0>)\n",
      "2.0677358508110046\n",
      "tensor(0.0743, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0834, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0981, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0375, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0887, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0697, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0842, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
      "2.016823783516884\n",
      "tensor(0.1033, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1066, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0718, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0606, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m path_model \u001b[38;5;241m=\u001b[39m models_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(path_model, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbatim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[127], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, loss_fn, optimizer, device, num_epochs, path_model, verbatim)\u001b[0m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, labels)\n\u001b[1;32m     30\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 3. Optimizer zero grad\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# ...\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 5. Optimizer step\u001b[39;00m\n",
      "File \u001b[0;32m~/mestrado/cv/virtualenv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mestrado/cv/virtualenv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ProfessorsCNN()\n",
    "dataset = SolarPanelDataset(train_df.sample(200), '/home/victor/mestrado/cv/project/CV-Project/SolarPanelSoilingImageDataset/Solar_Panel_Soiling_Image_dataset/PanelImages/')\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "loss_fn = torch.nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "num_epochs = 10\n",
    "path_model = models_dir + f'/{model.__class__.__name__}_{loss_fn.__class__.__name__}'\n",
    "os.makedirs(path_model, exist_ok=True)\n",
    "train(model, train_dataloader, loss_fn, optimizer, device, num_epochs, path_model, verbatim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "c1LvPvV6le56"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(32, 3, 3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode the input image\n",
    "        encoded = self.encoder(x)\n",
    "\n",
    "        # Reconstruct the original image from the compressed representation\n",
    "        reconstructed = self.decoder(encoded)\n",
    "\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "8rpJuShwm6wk"
   },
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder()\n",
    "autoencoder = autoencoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "0MzpTsnAmudz"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ahlwrs_5oCq6",
    "outputId": "33aa85ba-fa67-4d1f-e978-54fca7131cb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (3): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wfeL_8pymy_e",
    "outputId": "99a48014-be10-467c-9829-8ed567ede2c6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa\n",
      "bbb\n",
      "Epoch: 0/10, Iteration: 0/96 Loss: 1.028418046189472e-05\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for i, images in enumerate(train_dataloader):\n",
    "\n",
    "        images = images.to(device)\n",
    "        # Reconstruct the images\n",
    "        reconstructions = autoencoder(images)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_function(reconstructions, images)\n",
    "\n",
    "        print('aaa')\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('bbb')\n",
    "\n",
    "        # Print the loss\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: {}/10, Iteration: {}/{} Loss: {}'.format(epoch, i, len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtQyzkNenEvW",
    "outputId": "bff3c965-ce31-4cb0-d6f5-e005051dca7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCrcunjEoPrL",
    "outputId": "b23457ff-efc0-405e-a754-758beb1dc16b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (3): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "YKmpdLU4obYj"
   },
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "6JXgARz6o2qj"
   },
   "outputs": [],
   "source": [
    "for test_batch, _ in test_dataloader:\n",
    "    test_batch = test_batch.to(device)\n",
    "    predicted_image_batch = autoencoder(test_batch)\n",
    "    encoded_batch = autoencoder.encoder(test_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "iIKHssGLogv5"
   },
   "outputs": [],
   "source": [
    "\n",
    "def tensor_to_image(tensor):\n",
    "\n",
    "    \"\"\"\n",
    "    Converts a tensor to an image.\n",
    "\n",
    "    Args:\n",
    "        tensor: A tensor of shape (C, H, W).\n",
    "\n",
    "    Returns:\n",
    "        An image.\n",
    "    \"\"\"\n",
    "\n",
    "    image = tensor.cpu().detach().numpy()\n",
    "    image = image.transpose(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    image = (image * 255).astype('uint8')  # Convert pixel values from [0, 1] to [0, 255]\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAZMTn-isABI",
    "outputId": "d8612fc6-b2ce-442b-bd89-92ee187aaa89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_image_batch[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhbMCy8EroRo",
    "outputId": "adea34e4-abf8-4398-f591-4e9138d87e41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 16])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(encoded_batch[2].transpose(0, 1).transpose(0,2),dim=1).unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "EuFYpVL-qxRa",
    "outputId": "5afd9947-bb4f-487c-9bf8-d368f594d7f6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(1, 1))\n",
    "plt.imshow(tensor_to_image(torch.mean(encoded_batch[5].transpose(0, 1).transpose(0,2),dim=1).unsqueeze(0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "nNbf9tYvojl-",
    "outputId": "172ece9d-14dc-4ca3-92b8-40a8e017e2ef"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(1, 1))\n",
    "plt.imshow(tensor_to_image(predicted_image_batch[3]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "YQ6EDTELpkZg",
    "outputId": "da2684c4-d268-4f65-a360-78bc2186d36f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(1, 1))\n",
    "plt.imshow(tensor_to_image(test_batch[3]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vYUgNJypt8t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
