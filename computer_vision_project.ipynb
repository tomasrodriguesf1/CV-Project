{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EVZbTfBslbWa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/codes/python_env/virtualenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/victor/codes/python_env/virtualenv/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available with 1 device(s).\n",
      "Current GPU: NVIDIA GeForce GTX 1660\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"GPU is available with {num_gpus} device(s).\")\n",
    "    \n",
    "    # Get the name of the current GPU\n",
    "    current_gpu = torch.cuda.get_device_name(0)  # Assuming the first GPU is used\n",
    "    print(f\"Current GPU: {current_gpu}\")\n",
    "else:\n",
    "    print(\"GPU is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use the first available GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # If no GPU is available, use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "models_dir = current_directory + '/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "df = pd.read_csv('filtered_data.csv')\n",
    "images_dir = '/run/media/victor/victor/cv_project/SolarPanelSoilingImageDataset/Solar_Panel_Soiling_Image_dataset/PanelImages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T15:58:57.402078300Z",
     "start_time": "2023-11-22T15:58:57.190041400Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mSolarPanelSoilingDataset\u001B[39;00m(\u001B[43mDataset\u001B[49m):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, data_dir, dataframe, transform\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, augmentation_transform\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m      3\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_dir \u001B[38;5;241m=\u001B[39m data_dir\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class SolarPanelDataset(data.Dataset):\n",
    "    #data_dir: The directory containing the image files\n",
    "    #image_shape: The shape of the images (height, width, channels)\n",
    "    #augmentation_copies: The number of copies of each image to create for data augmentation\n",
    "    def __init__(self, df, data_dir, image_shape=(3, 192, 192), augmentation_copies=1):\n",
    "        self.copies = 1 + augmentation_copies\n",
    "        self.image_shape = image_shape\n",
    "        self.df = df #df: A pandas DataFrame containing the image filenames and labels\n",
    "        self.labels = []\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((self.image_shape[1], self.image_shape[2])),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.augmentation_transform = self.make_augmentation_transform()\n",
    "        self.dataset_size = len(df)\n",
    "        self.images = torch.zeros((self.dataset_size, *self.image_shape), dtype=torch.float32)\n",
    "\n",
    "        for i, img_name in enumerate(self.df['original_title']):\n",
    "            img_path = os.path.join(data_dir, img_name)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            self.images[i] = image\n",
    "            label = df.iloc[i]['loss_percentage']\n",
    "            self.labels.append(label)  # Add corresponding label here\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)*self.copies\n",
    "\n",
    "    def make_augmentation_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.2),\n",
    "            transforms.RandomVerticalFlip(p=0.2),\n",
    "            transforms.RandomRotation(degrees=20),\n",
    "            transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.1, 0.1)),\n",
    "            transforms.GaussianBlur(kernel_size=3),  # You can adjust the kernel size\n",
    "            transforms.RandomApply([transforms.Lambda(lambda x: x + 0.01 * torch.randn_like(x))], p=0.2),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if (idx >= len(self.images) ):\n",
    "            image = self.images[idx % len(self.images)]\n",
    "            label = self.labels[idx % len(self.labels)]\n",
    "            return self.augmentation_transform(image), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(32400, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_2, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Batch normalization for improved training stability\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(128)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 24 * 24, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.sigmoid_layer = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.leaky_relu(self.batch_norm1(self.conv1(x))))\n",
    "        x = self.pool(F.leaky_relu(self.batch_norm2(self.conv2(x))))\n",
    "        x = self.pool(F.leaky_relu(self.batch_norm3(self.conv3(x))))\n",
    "\n",
    "        x = x.view(-1, 256 * 24 * 24)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to save the model\n",
    "def saveModel(model:nn.Module, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Training function. We simply have to loop over our data iterator and feed the inputs to the network and optimize.\n",
    "def train(model: torch.nn.Module,\n",
    "          dataloader: torch.utils.data.DataLoader,\n",
    "          valloader: torch.utils.data.DataLoader,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          metric: torchmetrics.Metric,\n",
    "          device: torch.device,\n",
    "          num_epochs,\n",
    "          path_model,\n",
    "          report: pd.DataFrame,\n",
    "          verbatim):\n",
    "    model.to(device)\n",
    "    metric.to(device)\n",
    "    best_accuracy = 0.0\n",
    "    best_loss = 0.0\n",
    "    best_epoch = 0\n",
    "    metric_algorithm = metric.__class__.__name__\n",
    "    loss_algorithm = loss_fn.__class__.__name__\n",
    "    best_metric = 100\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        model.train()\n",
    "        metric.reset()\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        for ibatch, (images, labels) in enumerate(dataloader, 0):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(images)\n",
    "            y_pred = y_pred.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            metric(y_pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate and accumulate accuracy metric across all batches\n",
    "            #y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            #train_acc += (y_pred_class == labels).sum().item()/len(y_pred)\n",
    "\n",
    "        # Adjust metrics to get average loss and accuracy per batch\n",
    "        train_loss = train_loss / len(dataloader)\n",
    "\n",
    "        # we want to save the model if the accuracy is the best\n",
    "\n",
    "        path = \"./myModel_\" +str(epoch)+ \".pth\"\n",
    "        #saveModel(model, path = path)\n",
    "\n",
    "        train_metric = metric.compute().item()/len(dataloader)\n",
    "        report_line = {'epoch': epoch,\n",
    "                       'loss': train_loss,\n",
    "                       'metric_algorithm': metric_algorithm,\n",
    "                       'metric': train_metric,\n",
    "                       'loss_algorithm': loss_algorithm,\n",
    "                       'mode': 'training'\n",
    "                      }\n",
    "        report = pd.concat([report, pd.DataFrame([report_line])], ignore_index=True)\n",
    "\n",
    "        if train_metric < best_metric:\n",
    "            path = str(path_model + '/bestModel.pth')\n",
    "            saveModel(model, path = path)\n",
    "            best_loss = train_loss\n",
    "            best_metric = train_metric\n",
    "            best_epoch = epoch\n",
    "            if verbatim:\n",
    "              print('Best Epoch #', epoch,' Loss=', best_loss, \" mean MAE(%)=\", train_metric)\n",
    "                \n",
    "        model.eval()\n",
    "        metric.reset()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                y_pred = model(images)\n",
    "                y_pred = y_pred.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
    "                val_loss += loss_fn(y_pred, labels).item()\n",
    "                metric(y_pred, labels)\n",
    "    \n",
    "        val_loss /= len(valloader)\n",
    "        val_metric = metric.compute().item()/len(valloader)\n",
    "        report_line = {'epoch': epoch,\n",
    "                       'loss': val_loss,\n",
    "                       'metric_algorithm': metric_algorithm,\n",
    "                       'metric': val_metric,\n",
    "                       'loss_algorithm': loss_algorithm,\n",
    "                       'mode': 'validation'\n",
    "                      }\n",
    "\n",
    "        print(val_metric)\n",
    "        report = pd.concat([report, pd.DataFrame([report_line])], ignore_index=True)\n",
    "    return best_loss, best_accuracy, best_epoch, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch # 0  Loss= 0.13654504550827873  mean MAE(%)= 0.018595192167494033\n",
      "0.07692307692307693\n",
      "Best Epoch # 1  Loss= 0.1354757525499851  mean MAE(%)= 0.01586507899420602\n",
      "0.07692307692307693\n",
      "0.07692307692307693\n",
      "0.07692307692307693\n",
      "0.07692307692307693\n",
      "0.07692307692307693\n",
      "0.07692307692307693\n",
      "0.07692307692307693\n",
      "0.07692307692307693\n",
      "0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "gc.collect() # Python thing\n",
    "\n",
    "model = CNN_2()\n",
    "metric = torchmetrics.MeanAbsolutePercentageError()\n",
    "train_dataset = SolarPanelDataset(train_df.sample(2000), images_dir)\n",
    "val_dataset = SolarPanelDataset(val_df.sample(400), images_dir)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "loss_fn = torch.nn.MSELoss().to(device)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "num_epochs = 10\n",
    "path_model = models_dir + f'/{model.__class__.__name__}_{loss_fn.__class__.__name__}'\n",
    "os.makedirs(path_model, exist_ok=True)\n",
    "report = pd.DataFrame()\n",
    "_, _, _, report = train(model, train_dataloader, val_dataloader, loss_fn, optimizer, metric, device, num_epochs, path_model, report, verbatim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='epoch', ylabel='metric'>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(data=report, x='epoch', y='metric', hue='mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model with the test dataset and print the accuracy for the test images\n",
    "def test(model: torch.nn.Module,\n",
    "         dataloader: torch.utils.data.DataLoader,\n",
    "         loss_fn: torch.nn.Module,\n",
    "         metric: torchmetrics.Metric,\n",
    "         device: torch.device,\n",
    "         verbatim = True):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    metric = metric.to(device)\n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss = 0\n",
    "    test_metric = 100\n",
    "    pred_labels = []\n",
    "    label_list = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            y_pred = model(images)\n",
    "            y_pred = y_pred.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
    "            pred_labels+=y_pred.tolist()\n",
    "            label_list += labels.tolist()\n",
    "            \n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            test_loss += loss.item()\n",
    "            metric.update(y_pred, labels)\n",
    "\n",
    "    eval_df = pd.DataFrame({'labels': label_list, 'predictions': pred_labels})\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_metric = metric.compute()\n",
    "\n",
    "    if verbatim:\n",
    "      print(\"Loss =\", test_loss, f'  Metric ({metric.__class__.__name__})=', test_metric.item())\n",
    "    return pred_labels, test_loss, test_metric, eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.1320844143629074   Metric (MeanAbsolutePercentageError)= 1.0\n"
     ]
    }
   ],
   "source": [
    "metric = torchmetrics.MeanAbsolutePercentageError()\n",
    "test_dataset = SolarPanelDataset(test_df.sample(200), images_dir, augmentation_copies=0)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "_, _, _, eval_df = test(model, test_dataloader, loss_fn, metric, device, verbatim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.672452</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.395786</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018813</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.595761</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017543</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.754628</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.051934</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.013678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.037478</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.205031</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       labels  predictions\n",
       "0    0.672452          0.0\n",
       "1    0.395786          0.0\n",
       "2    0.018813          0.0\n",
       "3    0.595761          0.0\n",
       "4    0.017543          0.0\n",
       "..        ...          ...\n",
       "195  0.754628          0.0\n",
       "196  0.051934          0.0\n",
       "197  0.013678          0.0\n",
       "198  0.037478          0.0\n",
       "199  0.205031          0.0\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['diff'] = eval_df['labels'] - eval_df['predictions']\n",
    "eval_df['diff'] = eval_df['diff'].abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "8rpJuShwm6wk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df.plot(kind='hist', y='diff', bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vYUgNJypt8t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
